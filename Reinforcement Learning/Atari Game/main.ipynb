{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atari Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries\n",
    "\n",
    "It is asumed that env from environment.waml is created.\n",
    "\n",
    "If not, use: conda env create --file=environment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13613357404959763401\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5717884928\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6573289936603156003\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(\"Num GPUs Available: \", len(tensorflow.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test random enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v4')\n",
    "#env = gym.make('SpaceInvaders-v4', render_mode=\"human\")\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "height, width, channels, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaqu\\anaconda3\\envs\\gym\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:60.0\n",
      "Episode:2 Score:160.0\n",
      "Episode:3 Score:140.0\n",
      "Episode:4 Score:95.0\n",
      "Episode:5 Score:310.0\n",
      "Episode:6 Score:100.0\n",
      "Episode:7 Score:45.0\n",
      "Episode:8 Score:185.0\n",
      "Episode:9 Score:110.0\n",
      "Episode:10 Score:85.0\n",
      "Episode:11 Score:50.0\n",
      "Episode:12 Score:15.0\n",
      "Episode:13 Score:365.0\n",
      "Episode:14 Score:265.0\n",
      "Episode:15 Score:120.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 15\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        action = random.choice([0,1,2,3,4,5])\n",
    "        obs, reward, terminated, truncated , info = env.step(action)\n",
    "        done = truncated or terminated\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    with tensorflow.device('/gpu:0'):\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "        model.add(Convolution2D(50, (4,4), strides=(2,2), activation='relu'))\n",
    "        model.add(Convolution2D(50, (3,3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(400, activation='relu'))\n",
    "        model.add(Dense(300, activation='relu'))\n",
    "        model.add(Dense(200, activation='relu'))\n",
    "        model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 3, 51, 39, 32)     6176      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 3, 24, 18, 50)     25650     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 22, 16, 50)     22550     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 52800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 400)               21120400  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 300)               120300    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 200)               60200     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 1206      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,356,482\n",
      "Trainable params: 21,356,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(height, width, channels, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', \n",
    "                                  value_max=1., value_min=.1, \n",
    "                                  value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "  422/10000: episode: 1, duration: 7.987s, episode steps: 422, steps per second:  53, episode reward: 105.000, mean reward:  0.249 [ 0.000, 30.000], mean action: 2.637 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joaqu\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1140/10000: episode: 2, duration: 138.354s, episode steps: 718, steps per second:   5, episode reward: 105.000, mean reward:  0.146 [ 0.000, 30.000], mean action: 2.362 [0.000, 5.000],  loss: 3.670008, mean_q: 2.365392, mean_eps: 0.903700\n",
      " 1754/10000: episode: 3, duration: 548.849s, episode steps: 614, steps per second:   1, episode reward: 135.000, mean reward:  0.220 [ 0.000, 30.000], mean action: 2.445 [0.000, 5.000],  loss: 0.830860, mean_q: 1.892055, mean_eps: 0.869815\n",
      " 2538/10000: episode: 4, duration: 698.695s, episode steps: 784, steps per second:   1, episode reward: 155.000, mean reward:  0.198 [ 0.000, 30.000], mean action: 2.546 [0.000, 5.000],  loss: 0.528761, mean_q: 2.291377, mean_eps: 0.806905\n",
      " 3152/10000: episode: 5, duration: 546.985s, episode steps: 614, steps per second:   1, episode reward: 80.000, mean reward:  0.130 [ 0.000, 20.000], mean action: 2.536 [0.000, 5.000],  loss: 0.212084, mean_q: 1.889015, mean_eps: 0.743995\n",
      " 3757/10000: episode: 6, duration: 539.715s, episode steps: 605, steps per second:   1, episode reward: 65.000, mean reward:  0.107 [ 0.000, 20.000], mean action: 2.494 [0.000, 5.000],  loss: 0.152655, mean_q: 1.787602, mean_eps: 0.689140\n",
      " 4525/10000: episode: 7, duration: 685.940s, episode steps: 768, steps per second:   1, episode reward: 135.000, mean reward:  0.176 [ 0.000, 30.000], mean action: 2.382 [0.000, 5.000],  loss: 0.269696, mean_q: 1.778356, mean_eps: 0.627355\n",
      " 5056/10000: episode: 8, duration: 472.658s, episode steps: 531, steps per second:   1, episode reward: 50.000, mean reward:  0.094 [ 0.000, 20.000], mean action: 2.360 [0.000, 5.000],  loss: 0.104118, mean_q: 1.372921, mean_eps: 0.568900\n",
      " 5704/10000: episode: 9, duration: 576.397s, episode steps: 648, steps per second:   1, episode reward: 75.000, mean reward:  0.116 [ 0.000, 25.000], mean action: 2.602 [0.000, 5.000],  loss: 0.131759, mean_q: 0.794451, mean_eps: 0.515845\n",
      " 6098/10000: episode: 10, duration: 349.731s, episode steps: 394, steps per second:   1, episode reward: 45.000, mean reward:  0.114 [ 0.000, 15.000], mean action: 2.350 [0.000, 5.000],  loss: 0.142169, mean_q: 1.062137, mean_eps: 0.468955\n",
      " 6720/10000: episode: 11, duration: 554.556s, episode steps: 622, steps per second:   1, episode reward: 15.000, mean reward:  0.024 [ 0.000, 15.000], mean action: 2.605 [0.000, 5.000],  loss: 0.081522, mean_q: 0.938536, mean_eps: 0.423235\n",
      " 7378/10000: episode: 12, duration: 585.361s, episode steps: 658, steps per second:   1, episode reward: 150.000, mean reward:  0.228 [ 0.000, 30.000], mean action: 2.397 [0.000, 5.000],  loss: 0.184577, mean_q: 1.445691, mean_eps: 0.365635\n",
      " 7754/10000: episode: 13, duration: 335.049s, episode steps: 376, steps per second:   1, episode reward: 35.000, mean reward:  0.093 [ 0.000, 20.000], mean action: 1.649 [0.000, 5.000],  loss: 0.263249, mean_q: 1.917873, mean_eps: 0.319105\n",
      " 8097/10000: episode: 14, duration: 305.537s, episode steps: 343, steps per second:   1, episode reward: 10.000, mean reward:  0.029 [ 0.000,  5.000], mean action: 2.437 [0.000, 5.000],  loss: 0.062040, mean_q: 1.508410, mean_eps: 0.286750\n",
      " 8752/10000: episode: 15, duration: 583.760s, episode steps: 655, steps per second:   1, episode reward: 105.000, mean reward:  0.160 [ 0.000, 30.000], mean action: 2.058 [0.000, 5.000],  loss: 0.146371, mean_q: 0.346986, mean_eps: 0.241840\n",
      " 9310/10000: episode: 16, duration: 495.044s, episode steps: 558, steps per second:   1, episode reward: 90.000, mean reward:  0.161 [ 0.000, 25.000], mean action: 1.787 [0.000, 5.000],  loss: 0.231070, mean_q: 0.540722, mean_eps: 0.187255\n",
      "done, took 8036.838 seconds\n"
     ]
    }
   ],
   "source": [
    "with tensorflow.device('/gpu:0'):\n",
    "    model = build_model(height, width, channels, actions)\n",
    "    dqn = build_agent(model, actions)\n",
    "    dqn.compile(Adam(lr=1e-4))\n",
    "    dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('atari_weights.h5f',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joaqu\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "c:\\Users\\joaqu\\anaconda3\\envs\\gym\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "c:\\Users\\joaqu\\anaconda3\\envs\\gym\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:289: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 85.000, steps: 628\n",
      "Episode 2: reward: 175.000, steps: 965\n",
      "Episode 3: reward: 30.000, steps: 530\n",
      "Episode 4: reward: 40.000, steps: 643\n",
      "Episode 5: reward: 15.000, steps: 501\n",
      "Episode 6: reward: 15.000, steps: 658\n",
      "Episode 7: reward: 20.000, steps: 611\n",
      "Episode 8: reward: 115.000, steps: 917\n",
      "Episode 9: reward: 80.000, steps: 734\n",
      "Episode 10: reward: 155.000, steps: 901\n",
      "73.0\n"
     ]
    }
   ],
   "source": [
    "env1 = gym.make('SpaceInvaders-v4', render_mode = 'human')\n",
    "height, width, channels = env1.observation_space.shape\n",
    "actions = env1.action_space.n\n",
    "model1 = build_model(height, width, channels, actions)\n",
    "dqn1 = build_agent(model1, actions)\n",
    "dqn1.compile(Adam(learning_rate=1e-4))\n",
    "dqn1.load_weights('atari_weights.h5f')\n",
    "\n",
    "scores = dqn1.test(env1, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
